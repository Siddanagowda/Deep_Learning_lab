{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b27d45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Program - 3\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_d(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.w_i_h = np.random.randn(input_size, hidden_size)\n",
    "        self.b_h = np.random.randn(1, hidden_size)\n",
    "        self.w_h_o = np.random.randn(hidden_size, output_size)\n",
    "        self.b_o = np.random.randn(1, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h_a = np.dot(x, self.w_i_h) + self.b_h\n",
    "        self.h_o = sigmoid(self.h_a)\n",
    "\n",
    "        self.o_a = np.dot(self.h_o, self.w_h_o) + self.b_o\n",
    "        self.o_o = softmax(self.o_a)\n",
    "\n",
    "        return self.o_o\n",
    "\n",
    "    def backward(self, x, y, output, lr):\n",
    "        m = y.shape[0]\n",
    "        d_o = output - y\n",
    "        e_h = d_o.dot(self.w_h_o.T)\n",
    "        d_h = e_h * sigmoid_d(self.h_o)\n",
    "\n",
    "        self.w_h_o -= self.h_o.T.dot(d_o) * lr / m\n",
    "        self.b_o -= np.sum(d_o, axis=0, keepdims=True) * lr /m\n",
    "        self.w_i_h -= x.T.dot(d_h) *lr / m\n",
    "        self.b_h -= np.sum(d_h, axis=0, keepdims=True) * lr /m\n",
    "\n",
    "    def loss_calculation(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "        return loss\n",
    "\n",
    "\n",
    "x = np.array([[0,0], [1,0], [0,1], [1,1]])\n",
    "y = np.array([[1,0,0], [0,1,0], [0,1,0], [0,0,1]])\n",
    "\n",
    "input_size, hidden_size, output_size = 2, 4, 3\n",
    "lr = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# train\n",
    "for i in range(epochs):\n",
    "    output = nn.forward(x)\n",
    "    nn.backward(x, y, output, lr)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        loss = nn.loss_calculation(y, output)\n",
    "        print(f\"Epoch {i} | Loss {loss:.4f}\")\n",
    "\n",
    "print(\"predicting outputs \")\n",
    "prediction = nn.forward(x)\n",
    "print(prediction)\n",
    "\n",
    "print(\"\\npredicting classes \")\n",
    "print(np.argmax(prediction, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0bd99",
   "metadata": {},
   "source": [
    "# Program 3: Multi-Class Classification Using a Neural Network\n",
    "\n",
    "This program implements a simple neural network to perform multi-class classification. It uses Python's `numpy` library for numerical computations and demonstrates forward propagation, backpropagation, loss calculation, and training of the network.\n",
    "\n",
    "---\n",
    "\n",
    "## **Features**\n",
    "\n",
    "1. **Neural Network Architecture**:\n",
    "   - Input Layer: 2 neurons (for input features).\n",
    "   - Hidden Layer: 4 neurons.\n",
    "   - Output Layer: 3 neurons (for multi-class output).\n",
    "\n",
    "2. **Activation Functions**:\n",
    "   - Sigmoid function for the hidden layer.\n",
    "   - Softmax function for the output layer.\n",
    "\n",
    "3. **Loss Function**:\n",
    "   - Cross-entropy loss is used to evaluate the performance of the network.\n",
    "\n",
    "4. **Training**:\n",
    "   - The network is trained using backpropagation with a learning rate of 0.1 for 10,000 epochs.\n",
    "\n",
    "5. **Prediction**:\n",
    "   - After training, the network predicts the class probabilities and the corresponding class labels for the given inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Code Overview**\n",
    "\n",
    "### **Imports**\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "numpy: For numerical computations.\n",
    "\n",
    "### Activation Functions\n",
    "#### Sigmoid Function\n",
    "```\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "Maps input values to the range (0, 1).\n",
    "\n",
    "#### Sigmoid Derivative\n",
    "```\n",
    "def sigmoid_d(x):\n",
    "    return x * (1 - x)\n",
    "```\n",
    "Derivative of the sigmoid function, used in backpropagation.\n",
    "\n",
    "#### Softmax Function\n",
    "```\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "```\n",
    "Converts input values into probabilities that sum to 1 for multi-class classification.\n",
    "### Neural Network Class\n",
    "```\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.w_i_h = np.random.randn(input_size, hidden_size)\n",
    "        self.b_h = np.random.randn(1, hidden_size)\n",
    "        self.w_h_o = np.random.randn(hidden_size, output_size)\n",
    "        self.b_o = np.random.randn(1, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h_a = np.dot(x, self.w_i_h) + self.b_h\n",
    "        self.h_o = sigmoid(self.h_a)\n",
    "\n",
    "        self.o_a = np.dot(self.h_o, self.w_h_o) + self.b_o\n",
    "        self.o_o = softmax(self.o_a)\n",
    "\n",
    "        return self.o_o\n",
    "\n",
    "    def backward(self, x, y, output, lr):\n",
    "        m = y.shape[0]\n",
    "        d_o = output - y\n",
    "        e_h = d_o.dot(self.w_h_o.T)\n",
    "        d_h = e_h * sigmoid_d(self.h_o)\n",
    "\n",
    "        self.w_h_o -= self.h_o.T.dot(d_o) * lr / m\n",
    "        self.b_o -= np.sum(d_o, axis=0, keepdims=True) * lr / m\n",
    "        self.w_i_h -= x.T.dot(d_h) * lr / m\n",
    "        self.b_h -= np.sum(d_h, axis=0, keepdims=True) * lr / m\n",
    "\n",
    "    def loss_calculation(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "        return loss\n",
    "```\n",
    "#### Initialization:\n",
    "Randomly initializes weights and biases for the input-to-hidden and hidden-to-output layers.\n",
    "#### Forward Propagation:\n",
    "Computes the activations for the hidden and output layers using the sigmoid and softmax functions.\n",
    "#### Backward Propagation:\n",
    "Updates weights and biases using the error gradient and learning rate.\n",
    "#### Loss Calculation:\n",
    "Computes the cross-entropy loss to evaluate the network's performance.\n",
    "### Training and Prediction\n",
    "```\n",
    "x = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([[1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1]])\n",
    "\n",
    "input_size, hidden_size, output_size = 2, 4, 3\n",
    "lr = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the network\n",
    "for i in range(epochs):\n",
    "    output = nn.forward(x)\n",
    "    nn.backward(x, y, output, lr)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        loss = nn.loss_calculation(y, output)\n",
    "        print(f\"Epoch {i} | Loss {loss:.4f}\")\n",
    "\n",
    "# Predict outputs\n",
    "print(\"Predicting outputs:\")\n",
    "prediction = nn.forward(x)\n",
    "print(prediction)\n",
    "\n",
    "# Predict class labels\n",
    "print(\"\\nPredicting classes:\")\n",
    "print(np.argmax(prediction, axis=1))\n",
    "```\n",
    "#### Inputs and Outputs:\n",
    "x: Input features.\n",
    "y: One-hot encoded class labels.\n",
    "#### Training:\n",
    "The network is trained for 10,000 epochs with a learning rate of 0.1.\n",
    "#### Prediction:\n",
    "After training, the network predicts the class probabilities and the corresponding class labels.\n",
    "### Output\n",
    "#### Predicted Probabilities:\n",
    "\n",
    "The network outputs probabilities for each class for the given inputs.\n",
    "#### Predicted Class Labels:\n",
    "\n",
    "The network predicts the class labels corresponding to the highest probabilities.\n",
    "### How to Run\n",
    "Ensure you have Python installed along with the required library:\n",
    "\n",
    "Run the program in a Jupyter Notebook or any Python environment.\n",
    "\n",
    "View the predicted probabilities and class labels for the given inputs.\n",
    "\n",
    "### Example Output\n",
    "```\n",
    "Epoch 0 | Loss 1.2345\n",
    "Epoch 1000 | Loss 0.4567\n",
    "Epoch 2000 | Loss 0.2345\n",
    "...\n",
    "Epoch 9000 | Loss 0.1234\n",
    "\n",
    "Predicting outputs:\n",
    "[[0.98 0.01 0.01]\n",
    " [0.01 0.97 0.02]\n",
    " [0.01 0.97 0.02]\n",
    " [0.01 0.02 0.97]]\n",
    "\n",
    "Predicting classes:\n",
    "[0 1 1 2]\n",
    "```\n",
    "The outputs are probabilities for each class, and the predicted class labels are [0, 1, 1, 2].\n",
    "\n",
    "## Conclusion\n",
    "This program demonstrates how a simple neural network can be implemented to solve a multi-class classification problem. It showcases the use of forward propagation, backpropagation, loss calculation, and training to achieve the desired outputs.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
