{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbaa10f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Program - 2 -> cross entropy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1,keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1,keepdims=True)\n",
    "\n",
    "def entropy(y_pred, y_true):\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "    loss = -np.mean(np.sum(y_true * np.log(y_pred)))\n",
    "    return loss\n",
    "\n",
    "def entropy_d(y_pred, y_true):\n",
    "    return y_pred - y_true\n",
    "\n",
    "def forward(w, x):\n",
    "    logits = np.dot(x,w)\n",
    "    return softmax(logits)\n",
    "\n",
    "def backward(X, y, y_pred, lr):\n",
    "    grad = entropy_d(y_pred, y)\n",
    "    w_update = np.dot(X.T, grad) / X.shape[0]\n",
    "    return w_update * lr\n",
    "\n",
    "# Input features\n",
    "X = np.array([[1.0, 2.0], [1.5, 2.5], [2.0, 3.0]]) \n",
    "#weights\n",
    "weights = np.array([[0.2, -0.3, 0.5], [-0.5, 0.7, -0.2]])\n",
    "# One-hot encoded labels\n",
    "y_true = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]]) \n",
    "\n",
    "y_pred = forward(weights,X)\n",
    "loss = entropy(y_pred, y_true)\n",
    "lr=0.1\n",
    "w_updated = backward(X, y_true, y_pred, lr)\n",
    "\n",
    "print(y_pred)\n",
    "print(loss)\n",
    "print(w_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85115178",
   "metadata": {},
   "source": [
    "# Program 2: XOR Implementation Using a Neural Network\n",
    "\n",
    "This program implements a simple neural network to solve the XOR problem. It uses Python's `numpy` library for numerical computations and demonstrates forward propagation, backpropagation, and training of the network.\n",
    "\n",
    "---\n",
    "\n",
    "## **Features**\n",
    "\n",
    "1. **Neural Network Architecture**:\n",
    "   - Input Layer: 2 neurons (for XOR inputs).\n",
    "   - Hidden Layer: 4 neurons.\n",
    "   - Output Layer: 1 neuron (for XOR output).\n",
    "\n",
    "2. **Activation Function**:\n",
    "   - Sigmoid function is used for both the hidden and output layers.\n",
    "\n",
    "3. **Training**:\n",
    "   - The network is trained using backpropagation with a learning rate of 0.1 for 10,000 epochs.\n",
    "\n",
    "4. **Prediction**:\n",
    "   - After training, the network predicts the XOR outputs for the given inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Code Overview**\n",
    "\n",
    "### **Imports**\n",
    "```\n",
    "import numpy as np\n",
    "```\n",
    "numpy: For numerical computations.\n",
    "\n",
    "### Activation Functions\n",
    "#### Sigmoid Function\n",
    "```\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "Maps input values to the range (0, 1).\n",
    "\n",
    "#### Sigmoid Derivative\n",
    "```\n",
    "def sigmoid_d(x):\n",
    "    return x * (1 - x)\n",
    "```\n",
    "\n",
    "Derivative of the sigmoid function, used in backpropagation.\n",
    "\n",
    "#### Neural Network Class\n",
    "```\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.w_i_h = np.random.randn(input_size, hidden_size)\n",
    "        self.b_h = np.random.randn(1, hidden_size)\n",
    "        self.w_h_o = np.random.randn(hidden_size, output_size)\n",
    "        self.b_o = np.random.randn(1, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h_a = np.dot(x, self.w_i_h) + self.b_h\n",
    "        self.h_o = sigmoid(self.h_a)\n",
    "\n",
    "        self.o_a = np.dot(self.h_o, self.w_h_o) + self.b_o\n",
    "        self.o_o = sigmoid(self.o_a)\n",
    "\n",
    "        return self.o_o\n",
    "\n",
    "    def backward(self, x, y, output, lr):\n",
    "        e = y - output\n",
    "        d_o = e * sigmoid_d(output)\n",
    "        e_h = d_o.dot(self.w_h_o.T)\n",
    "        d_h = e_h * sigmoid_d(self.h_o)\n",
    "\n",
    "        self.w_h_o += self.h_o.T.dot(d_o) * lr\n",
    "        self.b_o += np.sum(d_o, axis=0, keepdims=True) * lr\n",
    "        self.w_i_h += x.T.dot(d_h) * lr\n",
    "        self.b_h += np.sum(d_h, axis=0, keepdims=True) * lr\n",
    "```\n",
    "##### Initialization:\n",
    "Randomly initializes weights and biases for the input-to-hidden and hidden-to-output layers.\n",
    "##### Forward Propagation:\n",
    "Computes the activations for the hidden and output layers using the sigmoid function.\n",
    "##### Backward Propagation:\n",
    "Updates weights and biases using the error gradient and learning rate.\n",
    "#### Training and Prediction\n",
    "```\n",
    "x = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "input_size, hidden_size, output_size = 2, 4, 1\n",
    "lr = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the network\n",
    "for i in range(epochs):\n",
    "    output = nn.forward(x)\n",
    "    nn.backward(x, y, output, lr)\n",
    "\n",
    "# Predict outputs\n",
    "print(\"Predicting outputs:\")\n",
    "print(nn.forward(x))\n",
    "```\n",
    "#### Inputs and Outputs:\n",
    "x: XOR inputs.\n",
    "y: XOR outputs.\n",
    "#### Training:\n",
    "The network is trained for 10,000 epochs with a learning rate of 0.1.\n",
    "#### Prediction:\n",
    "After training, the network predicts the XOR outputs for the given inputs.\n",
    "Output\n",
    "#### Predicted Outputs:\n",
    "The network outputs values close to [0, 1, 1, 0] for the XOR inputs [[0, 0], [1, 0], [0, 1], [1, 1]].\n",
    "#### How to Run\n",
    "Ensure you have Python installed along with the required library:\n",
    "\n",
    "Run the program in a Jupyter Notebook or any Python environment.\n",
    "\n",
    "View the predicted outputs for the XOR problem.\n",
    "\n",
    "#### Example Output\n",
    "```\n",
    "Predicting outputs:\n",
    "[[0.01]\n",
    " [0.98]\n",
    " [0.98]\n",
    " [0.02]]\n",
    "```\n",
    "The outputs are close to the expected XOR results [0, 1, 1, 0].\n",
    "## Conclusion\n",
    "This program demonstrates how a simple neural network can be implemented to solve the XOR problem. It showcases the use of forward propagation, backpropagation, and training to achieve the desired outputs.\n",
    "\n",
    "Similar code found with 1 license type - View matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef6778",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
