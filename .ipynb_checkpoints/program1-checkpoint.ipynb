{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71372d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### program - 1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def adjusted_lr(lr, ep, dr=0.1, de=10):\n",
    "    return lr * (1 / (1 + dr * (ep // de)))\n",
    "\n",
    "\n",
    "# inputs\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "function = [(sigmoid, \"sigmoid\"), (relu, \"relu\"), (tanh, \"tanh\")]\n",
    "\n",
    "for i, (func, title) in enumerate(function):\n",
    "    y = func(x)\n",
    "    axes[i].plot(x,y)\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "\n",
    "x_softmax = np.linspace(-2,2,5)\n",
    "y_softmax = softmax(x_softmax)\n",
    "axes[3].plot(x_softmax,y_softmax)\n",
    "axes[3].set_title(\"softmax\")\n",
    "axes[3].grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "lr = 0.1\n",
    "epoch = 20\n",
    "print(\"Adjusted learning rate \",adjusted_lr(lr, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea992f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epoch = 20\n",
    "\n",
    "x = np.linspace(0, epoch, epoch)\n",
    "y = np.zeros(epoch)\n",
    "\n",
    "for i in range(epoch):\n",
    "    y[i] = adjusted_lr(lr, i)\n",
    "\n",
    "plt.plot(x, y, marker='o')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('lr')\n",
    "plt.title('lr decay')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601ddf6",
   "metadata": {},
   "source": [
    "# Program 1: Activation Functions and Learning Rate Decay Visualization\n",
    "\n",
    "This program demonstrates the behavior of common activation functions used in deep learning and visualizes the decay of the learning rate over epochs. It uses Python libraries `numpy` and `matplotlib` for numerical computations and plotting.\n",
    "\n",
    "---\n",
    "\n",
    "## **Features**\n",
    "\n",
    "1. **Activation Functions**:\n",
    "   - Sigmoid\n",
    "   - ReLU (Rectified Linear Unit)\n",
    "   - Tanh (Hyperbolic Tangent)\n",
    "   - Softmax\n",
    "\n",
    "2. **Learning Rate Adjustment**:\n",
    "   - Implements a learning rate decay function to simulate the gradual reduction of the learning rate over training epochs.\n",
    "\n",
    "3. **Visualization**:\n",
    "   - Plots the graphs of activation functions.\n",
    "   - Visualizes the learning rate decay over epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Code Overview**\n",
    "\n",
    "### **Imports**\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "### Activation Functions\n",
    "#### Sigmoid Function\n",
    "```\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "Maps input values to the range (0, 1).\n",
    "\n",
    "#### ReLU Function\n",
    "```\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "```\n",
    "Outputs the input if positive; otherwise, outputs 0.\n",
    "\n",
    "#### Tanh Function\n",
    "```\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "```\n",
    "Maps input values to the range (-1, 1).\n",
    "\n",
    "#### Softmax Function\n",
    "```\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "```\n",
    "Converts input values into probabilities that sum to 1.\n",
    "\n",
    "#### Learning Rate Adjustment\n",
    "```\n",
    "def adjusted_lr(lr, ep, dr=0.1, de=10):\n",
    "    return lr * (1 / (1 + dr * (ep // de)))\n",
    "```\n",
    "Adjusts the learning rate (lr) based on the epoch (ep), decay rate (dr), and decay interval (de).\n",
    "\n",
    "#### Visualization\n",
    "#### Activation Functions\n",
    "```\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "function = [(sigmoid, \"sigmoid\"), (relu, \"relu\"), (tanh, \"tanh\")]\n",
    "\n",
    "for i, (func, title) in enumerate(function):\n",
    "    y = func(x)\n",
    "    axes[i].plot(x, y)\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid()\n",
    "\n",
    "x_softmax = np.linspace(-2, 2, 5)\n",
    "y_softmax = softmax(x_softmax)\n",
    "axes[3].plot(x_softmax, y_softmax)\n",
    "axes[3].set_title(\"softmax\")\n",
    "axes[3].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "Plots the graphs of sigmoid, ReLU, tanh, and softmax functions.\n",
    "### Learning Rate Decay\n",
    "```\n",
    "lr = 0.1\n",
    "epoch = 20\n",
    "\n",
    "x = np.linspace(0, epoch, epoch)\n",
    "y = np.zeros(epoch)\n",
    "\n",
    "for i in range(epoch):\n",
    "    y[i] = adjusted_lr(lr, i)\n",
    "\n",
    "plt.plot(x, y, marker='o')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('lr')\n",
    "plt.title('lr decay')\n",
    "plt.show()\n",
    "```\n",
    "Visualizes the decay of the learning rate over 20 epochs.\n",
    "\n",
    "### Output\n",
    "#### Activation Functions:\n",
    "Graphs of sigmoid, ReLU, tanh, and softmax functions.\n",
    "\n",
    "#### Learning Rate Decay:\n",
    "A plot showing the gradual reduction of the learning rate over epochs.\n",
    "\n",
    "### How to Run\n",
    "Ensure you have Python installed along with the required libraries:\n",
    "\n",
    "Run the program in a Jupyter Notebook or any Python environment.\n",
    "\n",
    "View the plots generated for activation functions and learning rate decay.\n",
    "\n",
    "### Example Output\n",
    "#### Activation Functions\n",
    "Sigmoid, ReLU, Tanh, and Softmax graphs.\n",
    "\n",
    "#### Learning Rate Decay\n",
    "A plot showing the learning rate decreasing over epochs.\n",
    "\n",
    "#### Conclusion\n",
    "This program provides a simple yet effective demonstration of key mathematical functions used in deep learning and their visualization. It also showcases how learning rate decay can be implemented and visualized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4800832d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
